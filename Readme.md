# AI Framework Evaluation Project

## Overview

This repository is dedicated to testing and evaluating different solutions for common problems in AI workflows. The primary focus is on assessing the usefulness of various LLM (Large Language Model) frameworks and tools in commercial projects.

## Project Description

We are conducting an evaluation of three specific LLM tools:

1. Dspy
2. LlamaParse
3. JambaAI

The evaluation is based on a use case involving the analysis of the "Deloitte US 2023 Audit Quality Report". The tasks include:

- Scraping the document
- Extracting various key elements such as table of contents, performance indicators, recommendations, challenges, opportunities, risks, insights, findings, and conclusions

## Repository Structure

This repository contains:

1. Code implementations using each tool
2. Evaluation criteria and methodology
3. Results and comparisons
4. Documentation on setup and usage

## Objectives

The main objectives of this project are:

1. To assess the suitability of each tool for the given tasks
2. To compare the efficiency and effectiveness of different LLM frameworks
3. To provide insights into the strengths and weaknesses of each approach
4. To guide decision-making for similar projects in commercial settings

## Contributing

We welcome contributions and feedback. Please refer to the contributing guidelines for more information on how to participate in this project.

## License
